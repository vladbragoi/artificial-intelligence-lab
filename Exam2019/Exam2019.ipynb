{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-LAB EXAM 2019\n",
    "\n",
    "### Delivery instructions\n",
    "Send an email with this notebook attached where:\n",
    "* Recipient: alessandro.farinelli@univr.it\n",
    "* Carbon copy (CC): riccardo.sartea@univr.it\n",
    "* Subjet: AI Exam 2019 - Matricola, Name Surname\n",
    "* Body: Matricola, Name Surname\n",
    "* Attachment: this notebook **renamed as MATRICOLA.ipynb**. To rename the notebook click on the top-left of the browser tab (on \"Exam2019\") and insert the new name\n",
    "\n",
    "Every cell must run without errors when executing the notebook from top to bottom. If this is not the case, your exam may be invalidated. **Do not forget to answer the question in each assignment!**. Also, **do not forget to save the notebook! (CTRL+S)**\n",
    "\n",
    "## Windy Gridworld environment\n",
    "\n",
    "The environment to use for this exam is **WindyGridworld** (taken from the book of Sutton and Barto)\n",
    "![WindyGridworld](images/WindyGridworld.png)\n",
    "\n",
    "The agent starts in cell $(3, 0)$ and has to reach the goal in $(3, 7)$. All cells are safe but there is a crosswind running upward through the middle of the grid. The available actions are:\n",
    "* *U*: 0\n",
    "* *R*: 1\n",
    "* *D*: 2\n",
    "* *L*: 3\n",
    "\n",
    "but in the middle region the resultant next states are shifted upward by a *wind* the strength of which varies from column to column (visible at the bottom of each column in the figure). For example if the agent is in position $(3, 8)$, then action *Left* would move it to position $(2, 7)$. Going *Down* from position $(1, 6)$ would move the agent to position $(0, 6)$ instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym\n",
    "import envs\n",
    "import numpy as np\n",
    "import utils.heuristics as heu\n",
    "from utils.fringe import FringeNode, PriorityFringe, QueueFringe\n",
    "from utils.funcs import plot, build_path\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Assuming the agent has full knowledge of the environment, find the optimal path from the agent starting position to the goal with an uninformed or informed search algorithm of your choice. Suppose a step cost of 1 for any action. You have to show the result as a tuple $(path, stats)$ in the following form:\n",
    "* *path* - tuple of state identifiers forming a path from the start state to the goal state. ``None`` if no solution is found\n",
    "* *stats* - tuple of:\n",
    "     * *time* - time elapsed between the start and the end of the algorithm\n",
    "     * *expc* - number of nodes explored. A node is considered as explored when it is analyzed (state goal check, expansion, etc.)\n",
    "     * *maxnodes* - maximum number of nodes in memory at the same time\n",
    "     \n",
    "Insert the function/s defining your algorithm of choice in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR ALGORITHM HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the code to execute your algorithm and to show the result in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  T  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "\n",
      "\n",
      "Execution time: 0.0053s\n",
      "N° of nodes explored: 42\n",
      "Max n° of nodes in memory: 65\n",
      "Solution: [(3, 1), (3, 2), (3, 3), (2, 4), (1, 5), (0, 6), (0, 7), (0, 8), (0, 9), (1, 9), (2, 9), (3, 9), (4, 9), (4, 8), (3, 7)]\n"
     ]
    }
   ],
   "source": [
    "envname = \"WindyGridworld-v0\"\n",
    "\n",
    "# Create and render the environment\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "\n",
    "### CALL YOUR ALGORITHM AND SHOW RESULTS + STATS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Motivate your choice for the algorithm used. Focus only on optimality guarantees and stats (do not worry about exact numbers for the stats, just the trend). Consider both *tree search* and *graph search* variants in your motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "--------- INSERT ANSWER HERE!!!!!!! ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Consider the **WindyGridworld** environment defined above. Assume the reward is:\n",
    "\n",
    "* <span style=\"color:red\">-1</span> for every step\n",
    "\n",
    "and the episode ends only when the goal state is reached. The action dynamics are deterministic, but the agent has no prior knowledge of the environment (there is no $T$ or $R$ function avaible for use unless the agent creates them online). It is possible however to sample the next state given the current state and an action with *sample(state, action)* (again, with a deterministic result).\n",
    "\n",
    "Find the policies for the **WindyGridworld** environment by using *Q-Learning* ($\\epsilon$-greedy) and *SARSA* ($\\epsilon$-greedy) reinforcement learning algorithms. The solution returned must be a tuple *(policy, episodes)* where:\n",
    "* *policy*: array of action identifiers where the $i$-th action refers to the $i$-th state\n",
    "* *episodes*: array of cumulative episodes where the $i$-th value refers to cumulative number of episodes **completed** up to the $i$-th step. Example: if you execute a total number of 10 steps and your agent completes an episode after 4 steps and another one after 9, the $episodes$ array will look like this: $[0, 0, 0, 1, 1, 1, 1, 1, 2, 2]$. This means that the first episodes length is $4$ whether the second episode length is $5$.\n",
    "\n",
    "The code used to call your algorithms is already inserted below, therefore, **pay attention to the interface (input/output) of the functions to implement!!!** In particular, **the agent is given a total number of steps to do, not a total number of episodes!!!**. Hence, if the parameter *steps* is $100$, the agent must stop after $100$ steps, regardless of how many episodes have been completed.\n",
    "\n",
    "Insert the $\\epsilon$-greedy function in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    return 0  # CHANGE THIS!!! YOU NEED TO IMPLEMENT THIS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your *Q-Learning* algorithm implementation in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, steps, alpha, gamma, epsilon):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        steps: number of steps for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        epsilon: exploration paramenter for epsilon-greedy\n",
    "    \n",
    "    Returns:\n",
    "        (policy, episodes): final policy, cumulative number of episodes completed at each step [array]\n",
    "    \"\"\"\n",
    "    return [], []  # CHANGE THIS!!! YOU NEED TO IMPLEMENT THIS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your *SARSA* algorithm implementation in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(environment, steps, alpha, gamma, epsilon):\n",
    "    \"\"\"\n",
    "    Performs the SARSA algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI gym environment\n",
    "        steps: number of steps for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        epsilon: exploration paramenter for epsilon-greedy\n",
    "    \n",
    "    Returns:\n",
    "        (policy, episodes): final policy, cumulative number of episodes completed at each step [array]\n",
    "    \"\"\"\n",
    "    return [], []  # CHANGE THIS!!! YOU NEED TO IMPLEMENT THIS FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes your two algorithms and plots a chart with the behavior of the agent in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  WindyGridworld-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  T  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9bd37d1f2374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Q-Learning epsilon greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m print(\"Q-Learning\\nExecution time: {0}s\\nPolicy:\\n{1}\\n\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n\u001b[0m\u001b[1;32m     23\u001b[0m     env.shape))))\n\u001b[1;32m     24\u001b[0m \u001b[0mq_learning_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Q-Learning\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "envname = \"WindyGridworld-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: \", envname)\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "# Learning parameters\n",
    "steps = 15000\n",
    "alpha = .5\n",
    "gamma = .9\n",
    "delta = 1e-3\n",
    "epsilon = .1\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Q-Learning epsilon greedy\n",
    "policy, episodes = q_learning(env, steps, alpha, gamma, epsilon)\n",
    "print(\"Q-Learning\\nExecution time: {0}s\\nPolicy:\\n{1}\\n\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n",
    "    env.shape))))\n",
    "q_learning_series = {\"x\": np.arange(steps), \"y\": episodes, \"label\": \"Q-Learning\"}\n",
    "\n",
    "# SARSA epsilon greedy\n",
    "policy, episodes = sarsa(env, steps, alpha, gamma, epsilon)\n",
    "print(\"SARSA\\nExecution time: {0}s\\nPolicy:\\n{1}\\n\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n",
    "    env.shape))))\n",
    "sarsa_series = {\"x\": np.arange(steps), \"y\": episodes, \"label\": \"SARSA\"}\n",
    "plot([q_learning_series, sarsa_series], \"Learning Performance\", \"Steps\", \"Episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "Explain the differences in the learning performance between the two algorithms by explicitly referring to the chart above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "--------- INSERT ANSWER HERE!!!!!!! ---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
