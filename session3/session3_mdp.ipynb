{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-LAB SESSION 3: Markov Decision Process\n",
    "\n",
    "In the third session we will work on the Markov decision process (MDP)\n",
    "\n",
    "## Lava environments\n",
    "\n",
    "The environments used are **LavaFloor** (visible in the figure) and its variations.\n",
    "![LavaFloor](images/lava.png)\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, there is a black pit of death. What a nice place to visit!\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure)\n",
    "![DynAct](images/dynact.png)\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving in the desired direction\n",
    "- $P(0.1)$ of moving in a direction 90° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "- <span style=\"color:orange\">-0.04</span> for each lava cell (L)\n",
    "- <span style=\"color:red\">-5</span> for the black pit (P). End of episode\n",
    "- <span style=\"color:green\">+1</span> for the treasure (G). End of episode\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "Your first assignment is to implement the *Value Iteration* algorithm on **LavaFloor**. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. After the correctness of your implementations have been assessed, you can run the algorithms on other four lava environments: **VeryBadLavaFloor**, **NiceLavaFloor**, **BiggerLavaFloor**, **HugeLavaFloor**\n",
    "\n",
    "### Hint\n",
    "\n",
    "The first version of your implementation should use for loops and such as in the pseudocode. Once the result is satisfying try to use as many numpy operations as possible to speed up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym\n",
    "import envs\n",
    "import numpy as np\n",
    "from utils.funcs import run_episode, plot\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function has to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters, gamma, delta):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: max iterations allowed\n",
    "        gamma: gamma value\n",
    "        delta: delta value\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    # v = np.zeros(environment.observation_space.n)#, dtype=\"double\")  # Initial policy\n",
    "    # iter = 0\n",
    "    # while True:\n",
    "    #     v1 = v.copy()\n",
    "    #     iter += 1\n",
    "    #     for current_state in range(environment.observation_space.n):\n",
    "    #         sum_list = [0 for a in environment.actions.keys()]\n",
    "    #         for a in environment.actions.keys():\n",
    "    #             for next_state in range(environment.observation_space.n):\n",
    "    #                 sum_list[a] += environment.T[current_state, a, next_state] \\\n",
    "    #                                * (environment.R[current_state, a, next_state] + gamma*v[next_state])\n",
    "    #         # print(sum_list)\n",
    "    #         v[current_state] = np.max(sum_list)\n",
    "    #     if np.max(np.abs(v-v1)) < delta or iter == maxiters: \n",
    "    #         break\n",
    "    # \n",
    "    # pi = np.zeros(environment.observation_space.n, dtype=\"int8\")\n",
    "    # for current_state in range(environment.observation_space.n):\n",
    "    #     sum_list = [0 for a in environment.actions.keys()]\n",
    "    #     for a in environment.actions.keys():\n",
    "    #         for next_state in range(environment.observation_space.n):\n",
    "    #                 sum_list[a] += environment.T[current_state, a, next_state] \\\n",
    "    #                                    * (environment.R[current_state, a, next_state] + gamma*v[next_state])\n",
    "    #     pi[current_state] = np.argmax(sum_list)\n",
    "    # return np.asarray(pi)\n",
    "     \n",
    "    v = np.zeros(environment.observation_space.n, dtype=\"int8\")\n",
    "    i = 0\n",
    "    tr = environment.T * environment.R\n",
    "    trm = np.sum(tr, axis=2)\n",
    "    while True:\n",
    "        v1 = np.copy(v)\n",
    "        i += 1\n",
    "        v = np.max(trm + np.sum(environment.T*gamma*v, axis=2), axis=1)\n",
    "        if i == maxiters or np.max(np.abs(v - v1)) < delta:\n",
    "            break\n",
    "\n",
    "    p = np.argmax(trm + np.sum(environment.T*gamma*v, axis=2), axis=1)\n",
    "    return np.asarray(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes and *Value Iteration* and prints the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  LavaFloor-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "\n",
      "Value Iteration:\n",
      "----------------------------------------------------------------\n",
      "Execution time: 0.0079s\n",
      "Policy:\n",
      "[['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  VeryBadLavaFloor-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "\n",
      "Value Iteration:\n",
      "----------------------------------------------------------------\n",
      "Execution time: 0.0041s\n",
      "Policy:\n",
      "[['R' 'R' 'R' 'D']\n",
      " ['D' 'L' 'R' 'L']\n",
      " ['R' 'R' 'R' 'L']]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  NiceLavaFloor-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "\n",
      "Value Iteration:\n",
      "----------------------------------------------------------------\n",
      "Execution time: 0.0095s\n",
      "Policy:\n",
      "[['R' 'U' 'L' 'U']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['U' 'U' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "delta = 1e-3\n",
    "gamma = .9\n",
    "maxiters = 50  # Max number of iterations to perform\n",
    "\n",
    "envname_list = [\"LavaFloor-v0\", \"VeryBadLavaFloor-v0\", \"NiceLavaFloor-v0\"]\n",
    "\n",
    "for envname in envname_list:\n",
    "    print(\"\\n----------------------------------------------------------------\")\n",
    "    print(\"\\tEnvironment: \", envname)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "    env = gym.make(envname)\n",
    "    env.render()\n",
    "\n",
    "    t = timer()\n",
    "    policy = value_iteration(env, maxiters, gamma, delta)\n",
    "\n",
    "    print(\"\\n\\nValue Iteration:\\n----------------------------------------------------------------\"\n",
    "          \"\\nExecution time: {0}s\\nPolicy:\\n{1}\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n",
    "                                                                                            env.rows, env.cols))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for *Value Iteration* can be found [here](results/value_iteration_results.txt)\n",
    "\n",
    "## Assignment 2\n",
    "\n",
    "Your first assignment is to implement the *Policy Iteration* algorithm on **LavaFloor**. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. After the correctness of your implementations have been assessed, you can run the algorithms on other four lava environments: **VeryBadLavaFloor**, **NiceLavaFloor**, **BiggerLavaFloor**, **HugeLavaFloor**\n",
    "\n",
    "### Hint\n",
    "\n",
    "The first version of your implementation should use for loops and such as in the pseudocode. Once the result is satisfying try to use as many numpy operations as possible to speed up the code.\n",
    "\n",
    "The following function has to be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_iteration(environment, pmaxiters, vmaxiters, gamma, delta):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: environment\n",
    "        pmaxiters: max iterations allowed for the policy improvement\n",
    "        vmaxiters: max iterations allowed for the policy evaluation\n",
    "        gamma: gamma value\n",
    "        delta: delta value\n",
    "    \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    # v = np.zeros(env.observation_space.n, dtype=\"double\")  # Initial policy\n",
    "    # p = np.zeros(env.observation_space.n, dtype=\"int8\")\n",
    "    # piter = 0\n",
    "    # while True:\n",
    "    #     p1 = np.copy(p)\n",
    "    #     piter += 1\n",
    "    #     viter = 0\n",
    "    #     # ---- Evaluate Policy\n",
    "    #     while True:\n",
    "    #         v1 = v.copy()\n",
    "    #         viter += 1\n",
    "    #         for s in range(env.observation_space.n): #itero sugli stati (for each s in S:)\n",
    "    #             sum_ = 0\n",
    "    #             # T(s, pi_s, s')(R(s, pi_s, s') + γV(s'))\n",
    "    #             for s1 in range(env.observation_space.n): #(for each s' in S:)\n",
    "    #                 sum_ += env.T[s, p[s], s1]*(env.R[s, p[s], s1] + gamma*v[s1]) \n",
    "    #             v[s] = sum_\n",
    "    #         if np.max(np.abs(v - v1)) < delta or viter == vmaxiters:\n",
    "    #             break\n",
    "    #     # ---- Improve Policy\n",
    "    #     for s in range(env.observation_space.n): #itero sugli stati (for each s in S:)\n",
    "    #         sum_list = []\n",
    "    #         for a in range(env.action_space.n): #(for each a in As:)\n",
    "    #             sum_ = 0\n",
    "    #             for s1 in range(env.observation_space.n): #(for each s' in S:)\n",
    "    #                 sum_ += env.T[s, a, s1]*(env.R[s, a, s1] + gamma*v[s1]) #T(s, a, s')(R(s, a, s') + γV(s'))\n",
    "    #             sum_list.append(sum_)\n",
    "    #         p[s] = np.argmax(sum_list)\n",
    "    #     if np.array_equal(p,p1) or piter == pmaxiters:\n",
    "    #         break\n",
    "    # \n",
    "    # return np.asarray(p)\n",
    "    p = np.zeros(environment.observation_space.n, dtype=\"int8\")  # Initial policy\n",
    "    v = np.zeros(environment.observation_space.n, dtype=\"int8\")\n",
    "    piter = 0\n",
    "    states = environment.observation_space.n\n",
    "    tr = environment.T * environment.R\n",
    "    trm = np.sum(tr, axis=2)\n",
    "    \n",
    "    while True:\n",
    "        p1 = np.copy(p)\n",
    "        piter += 1\n",
    "        viter = 0\n",
    "        while True:\n",
    "            v1 = np.copy(v)\n",
    "            viter += 1\n",
    "            v = (trm + np.sum(environment.T*gamma*v, axis=2))[np.arange(states), p]\n",
    "            if viter == vmaxiters or np.max(np.abs(v - v1)) < delta:\n",
    "                break\n",
    "        p = np.argmax(trm + np.sum(environment.T*gamma*v, axis=2), axis=1)\n",
    "        if piter == pmaxiters or np.array_equal(p, p1):\n",
    "            break\n",
    "    return np.asarray(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes and *Value Iteration* and prints the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  LavaFloor-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "\n",
      "Policy Iteration:\n",
      "----------------------------------------------------------------\n",
      "Execution time: 0.0071s\n",
      "Policy:\n",
      "[['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  VeryBadLavaFloor-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "\n",
      "Policy Iteration:\n",
      "----------------------------------------------------------------\n",
      "Execution time: 0.0052s\n",
      "Policy:\n",
      "[['R' 'R' 'R' 'D']\n",
      " ['D' 'L' 'R' 'L']\n",
      " ['R' 'R' 'R' 'L']]\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  NiceLavaFloor-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "\n",
      "\n",
      "Policy Iteration:\n",
      "----------------------------------------------------------------\n",
      "Execution time: 0.0117s\n",
      "Policy:\n",
      "[['R' 'L' 'L' 'U']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['R' 'L' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "delta = 1e-3\n",
    "gamma = .9\n",
    "pmaxiters = 50  # Max number of policy improvements to perform\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "\n",
    "envname_list = [\"LavaFloor-v0\", \"VeryBadLavaFloor-v0\", \"NiceLavaFloor-v0\"]\n",
    "\n",
    "for envname in envname_list:\n",
    "    print(\"\\n----------------------------------------------------------------\")\n",
    "    print(\"\\tEnvironment: \", envname)\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "    env = gym.make(envname)\n",
    "    env.render()\n",
    "\n",
    "    t = timer()\n",
    "    policy = policy_iteration(env, pmaxiters, vmaxiters, gamma, delta)\n",
    "\n",
    "    print(\"\\n\\nPolicy Iteration:\\n----------------------------------------------------------------\"\n",
    "          \"\\nExecution time: {0}s\\nPolicy:\\n{1}\".format(round(timer() - t, 4), np.vectorize(env.actions.get)(policy.reshape(\n",
    "                                                                                            env.rows, env.cols))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for *Policy Iteration* can be found [here](results/policy_iteration_results.txt)\n",
    "\n",
    "## Comparison\n",
    "\n",
    "The following code performs a comparison between *Value Iteration* and *Policy Iteration* by plotting the accumulated rewards of each episode with iterations in range $[1, 50]$. Try it on all the previous environments and also on **HugeLavaFloor** (might take a long time if not optimizied via numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------\n",
      "\tEnvironment:  HugeLavaFloor-v0\n",
      "----------------------------------------------------------------\n",
      "\n",
      "[['S' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L' 'L' 'P' 'L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'W' 'L' 'L' 'W' 'L' 'L' 'L']\n",
      " ['L' 'L' 'P' 'W' 'L' 'L' 'W' 'L' 'P' 'L']\n",
      " ['L' 'L' 'L' 'W' 'L' 'L' 'W' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'W' 'W' 'W' 'W' 'L' 'L' 'L']\n",
      " ['L' 'L' 'P' 'L' 'L' 'L' 'L' 'L' 'L' 'P']\n",
      " ['L' 'L' 'L' 'L' 'L' 'P' 'L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      " ['P' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'G']]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Value Iteration', max=51, style=ProgressStyle(description_wid…"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Policy Iteration', max=51, style=ProgressStyle(description_wi…"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "delta = 1e-3\n",
    "gamma = 0.9\n",
    "maxiters = 50  # Max number of iterations to perform\n",
    "\n",
    "envname = \"HugeLavaFloor-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: \", envname)\n",
    "print(\"----------------------------------------------------------------\\n\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "env.render()\n",
    "print(\"\\n\")\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, i, gamma, delta)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, i, vmaxiters, gamma, delta)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "print(\"Leaning rate comparison:\\nVI: {0}\\nPI: {1}\".format(virewards, pirewards))\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thhing is the global trend.\n",
    "\n",
    "### BiggerLavaFloor\n",
    "![Bigger](results/bigger.png)\n",
    "\n",
    "### HugeLavaFloor\n",
    "![Huge](results/huge.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
